---
title: "FLOWdash Willapa"
logo: assets/wdfw_logo_stacked_fullcolor.png
format: 
  dashboard:
    orientation: rows
theme: lux
embed-resources: true
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
#knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 13)

library("tidyverse", quietly = T)
theme_set(theme_minimal()) 

# #workbook of basic site/station metadata: `usgs_sites`
# readxl::read_excel("~/T/DFW-Team WDFW Watershed Synthesis - flow_trees_heat/usgs_sites_dailyQ_focal.xlsx") |> 
#   select(site_no, station_nm) |> 
#   mutate(station_nm = str_remove(station_nm, ", WA$") |> str_replace("RIVER", "R") |> str_to_title()) 

sites <- data.frame(
  site_no = c("12010000","12013500"),
  station_nm = c("Naselle R Near Naselle", "Willapa R Near Willapa")
)
  
q_obs_range <- c(as.Date("1979-01-01"), Sys.Date())


# # for ongoing dependent data (q_dv and bfs), could/should update relative to render date
# # possibly modifying 'rebuild' chunks to store as other than rds (csv or sqlite)?
# Sys.Date() - list.files("data", pattern = "usgs_dailyQ", full.names = T) |> 
#   file.info() |> mutate(d = date(mtime)) |> pull(d)


#build uniform object inserting NAs for missing obs during q_obs_range
#rebuild/overwrite y, m, and yday since `complete` inserts many NAs
#then calc per site per CALENDAR year 
# - cumulative sum of daily mean flow (weird but useful proxy for overall annual volume, 'wet/dry year')
# - 7day moving average of daily mean flow
usgs_q_dv <- list.files("data", pattern = "usgs_dailyQ", full.names = T) |> 
  map_df(~readRDS(.x)) |> 
  #group_by(site_no) |> summarise(dmin = min(date), dmax = max(date))
  tidyr::complete(site_no, date = full_seq(q_obs_range, 1))|> 
  mutate(
    year = as.character(year(date)), month = month(date), yday = yday(date)
    #,q_dv_mean = if_else(q_dv_mean < 0, NA_real_, q_dv_mean)
  ) |> 
  # mutate(
  #   q_dv_mean_sum = cumsum(q_dv_mean),
  #   #,q_dv_mean_7d = slider::slide_dbl(q_dv_mean, ~mean(., na.rm=T), .before = 3, .after = 3),
  #   .by = c(site_no, year)
  # ) |> 
  left_join(sites, by = "site_no") |> 
  select(site_no, station_nm, everything())
#add yday median across years of daily val
#could do 7day of this for pretty 'smooth' reference
usgs_q_dv <- bind_rows(
  usgs_q_dv,
  usgs_q_dv |>
    group_by(site_no, yday) |>
    summarise(
      year = "median",
      across(
        starts_with("q_dv"),
        ~median(.,na.rm=T)
      ),
      .groups = "drop"
    )
)

usgs_bfs <- readRDS("data/usgs_bfs_pred.rds")

nwfsc_st <- readRDS("data/nwfsc_st_pred.rds") |> 
  mutate(
    year = as.character(year(date)), month = month(date), yday = yday(date)
  )

rr_wb_coho <- readRDS("data/rr_wb_coho.rds")

rangeslider_thickness <- 0.05
#wacolors::pal_vector("ferries",n=46)
pal <- set_names(
  c("#241E33", "#24213E", "#222548", "#1F2A52", "#18305A",
    "#0F3661", "#043C66", "#00426B", "#00496F", "#004F72",
    "#005473", "#015A73", "#005F72", "#006371", "#006871",
    "#016C72", "#007172", "#007673", "#007A73", "#007F73",
    "#0D8474", "#1E8974", "#2D8D74", "#3A9275", "#459676",
    "#4F9A77", "#599F78", "#63A37A", "#6DA77B", "#77AB7E",
    "#81AF80", "#8BB283", "#95B687", "#9EBA8B", "#A7BE8F",
    "#B0C292", "#B8C693", "#C0CB92", "#C9CF90", "#D3D38D",
    "#DED78A", "#EADB86", "#F3DE81", "#FBE17B", "#FFE474",
    "#FFE56C", 
    "#B96000", #"#483778"
    "#DF3383", #"#000000FF",
    "#8A6172", "#8A6172"
    ),
  c(2024:1979, 'median', 'q_bfs_pred','q_bfs_pred05','q_bfs_pred95'))

plot_q_dv <- function(site, log10 = T){

  d <- usgs_q_dv |>
      filter(site_no==site) |>
      select(year, yday, q_dv_mean)
  
  if(site %in% unique(usgs_bfs$site_no)){
    d <- bind_rows(
      d,
      usgs_bfs |> 
        filter(site_no == site) |> 
        select(yday, starts_with("q_bfs")) |> 
        pivot_longer(cols = starts_with("q_bfs"), 
                     names_to = 'year',
                     values_to = 'q_dv_mean'
        )
      )
  }
  
  p <- d |> 
    plotly::plot_ly(
      type = 'scatter', mode = 'lines',
      name = ~year, x = ~yday, y = ~q_dv_mean,
      color = ~year, colors = pal
    ) |>
    plotly::layout(
      legend = list(traceorder = 'reversed'),
      xaxis = list(
        title = 'Day of year', 
        ticktext = format(seq.Date(as.Date("2024-01-01"),as.Date("2024-12-01"), by = 'month'),  format = '%b-%d'),
        tickvals = yday(seq.Date(as.Date("2024-01-01"),as.Date("2024-12-01"), by = 'month'))
        ),
      yaxis = list(title = 'cfs')
    )

  if (log10){
    p <- p |> plotly::layout(yaxis = list(type = "log"))
  }

  p
}
#plot_q_dv(sites$site_no[1])

plot_qmin <- function(site){
  usgs_q_dv |> 
    filter(site_no == site, year != "2024") |>
    slice_min(order_by = q_dv_mean, n = 1, by = year, with_ties = F) |>
    mutate(month = if_else(is.na(month), "med", month.abb[month]) |> 
             factor(levels = c(month.abb[6:10], "med"))) |> 
    select(year, month, min_cfs = q_dv_mean) |> 
    plotly::plot_ly(type = "bar", x = ~year, y = ~min_cfs, color = ~month) |> 
    plotly::rangeslider(thickness = rangeslider_thickness) |> 
    plotly::layout(
      legend = list(title='month'),
      xaxis = list(title='')
    )  
}

plot_qquant <- function(site){
  usgs_q_dv |> 
    filter(site_no == site, year != "2024") |>
    group_by(year) |> 
    summarise(
      q10 = quantile(q_dv_mean, p = 0.10),
      q05 = quantile(q_dv_mean, p = 0.05),
      q01 = quantile(q_dv_mean, p = 0.01),
      .groups = "drop") |>
    select(year, starts_with("q")) |>
    pivot_longer(-year, names_to = "quantile", values_to = "cfs") |>
    plotly::plot_ly(
      type = "bar", x = ~year, y = ~cfs,
      color = ~quantile, 
      colors = c(q10 = "darkblue",
                 q05 = "blue",
                 q01 = "lightblue"
                 )) |>
    plotly::rangeslider(thickness = rangeslider_thickness) |>
    plotly::layout(
      #barmode = 'overlay',
      xaxis = list(title='')
    )
} 

plot_tmax <- function(site){
  nwfsc_st |> 
    filter(site_no == site) |> 
    slice_max(order_by = st_pred, n = 1, by = year, with_ties = F) |>
    mutate(month = if_else(is.na(month), "med", month.abb[month]) |> 
             factor(levels = c(month.abb[6:10], "med"))) |> 
    select(year, month, max_degC = st_pred) |> 
    plotly::plot_ly( type = "bar", x = ~year, y = ~max_degC, color = ~month) |> 
    plotly::rangeslider(thickness = rangeslider_thickness) |> 
    plotly::layout(
      legend = list(title='month'),
      xaxis = list(title='')
    )  
}

plot_tover <- function(site){
  #degree thresholds are too hard-coded, but just to get started 
  nwfsc_st |> 
    filter(site_no == site) |> 
    group_by(year) |> 
    summarise(
      days_over18 = sum(st_pred > 18),
      days_over20 = sum(st_pred > 20),
      .groups = "drop") |>
    select(year, starts_with("days_over")) |>
    pivot_longer(-year, names_to = "over", values_to = "days") |> 
    plotly::plot_ly(
      type = "bar", x = ~year, y = ~days, 
      color = ~over, colors = c(days_over18 = "yellow", days_over20 = "orange")) |>
    plotly::rangeslider(thickness = rangeslider_thickness) |>
    plotly::layout(
      xaxis = list(title='')
    )
} 


#currently unused

plot_rr_ts <- function(site){
  rr_wb_coho |> 
    filter(site_no == site) |> 
    plotly::plot_ly(
      type = "scatter", mode = "lines+markers",
      x = ~year, y = ~val, color = ~var
      ) |> 
#    plotly::rangeslider() |> 
    plotly::layout(
      legend = list(title=''),
      xaxis = list(title=''),
      yaxis = list(title='')
    ) 
}
#using flow on warmest day; could should examine streamtemp on day of lowest flow?
plot_t_vs_q <- function(site){
  d <- left_join(
    nwfsc_st |> 
      filter(site_no == site) |> 
      slice_max(order_by = st_pred, n = 1, by = year, with_ties = F)
    , 
    usgs_q_dv
    ,
    by = c("site_no","station_nm","date","year","month","yday")    
  ) |>
    inner_join(
      rr_wb_coho |>
        filter(site_no == site 
               #,var == "fspt" 
               #,var %in% c("escp","fspt")
               ) |>
        mutate(year = as.character(year))
      , by = c("site_no", "year")
    ) |> 
    mutate(
      across(where(is.numeric), ~round(., digits = 1))
    )
  
  d |>
    plotly::plot_ly(
      type = "scatter", mode = "markers"
      ) |> 
    plotly::add_markers(
      x = ~q_dv_mean, y = ~st_pred,
      name = ~var, size = ~val, 
      marker = list(sizeref = 0.1),
      text = ~paste(
        '</br> Year: ', year,
        '</br> cfs: ', q_dv_mean,
        '</br> degC: ', st_pred,
        '</br> count: ', val
        )
      ) |> 
    plotly::layout(
      xaxis = list(title='cfs of obs Q on day of max pred. T'),
      yaxis = list(title='degC max pred. T')
    ) 
  
  # p <- map(
  #   c("escp","fspt"),
  #   function(v) {
  #     plotly::plot_ly(
  #       data = filter(d, var == v),
  #       x = ~q_dv_mean, y = ~st_pred, 
  #       name = v,
  #       size = ~val, text = ~year, 
  #       marker = list(sizeref = 0.1)
  #     ) #|> plotly::add_markers(name = v)
  #   })
  # 
  # plotly::subplot(p, nrows = 2)

}

```

```{r data_rebuild_q_dv, eval=FALSE}
# #not `complete()` here since no reason to store potentially lots of NA for long qobsrange
# walk(
#   sites$site_no
#   ,
#   ~dataRetrieval::readNWISdv(
#     .x, parameterCd = "00060",
#     startDate = q_obs_range[1],
#     endDate = q_obs_range[2]
#     ) |>
#     as_tibble() |>
#     mutate(year = year(Date), month = month(Date), yday = yday(Date)) |>
#     select(site_no, date = Date, year, month, yday, q_dv_mean = X_00060_00003) |>
#     saveRDS(paste0("data/usgs_dailyQ_", .x,".rds"))
# )
```

```{r data_rebuild_bfs, eval=FALSE}
# #add new USGS baseflow predictions...
# huc4 <- c("1710","1711")
# url <- paste0("https://wa.water.usgs.gov/projects/baseflows/out/bfprj_HUC",huc4,".csv")
# usgs_bfs <- map_df(url, ~readr::read_csv(.x)) |>
#   filter(SiteID %in% sites$site_no) |>
#   mutate(yday = yday(Date)) |>
#   select(
#     site_no = SiteID, date = Date, yday,
#     q_bfs_pred = Baseflow.cfs,
#     q_bfs_pred05 = StreamflowCB05.cfs,
#     q_bfs_pred95 = StreamflowCB95.cfs
#     ) |>
#     saveRDS(paste0("data/usgs_bfs_pred.rds"))

```

```{r data_rebuild_st_pred, eval=FALSE}
#could also think about adding/integrating NWM via AWS flow_trees_apps.qmd>>nwm_zarr_pull2 

# # #need to know gage HUC10 to figure out model prediction file; h10 lu not yet scripted, quick manual via
# # https://geo.wa.gov/datasets/waecy::national-watershed-boundary-dataset-wbd-hydrologic-unit-code-10-digit-basins-of-washington-state/explore?layer=11
# # # then need to know gage COMID since values are by COMID-day
# # # but gage may or may not actually be in the dataset for ... reasons?
# sites$COMID <- map_int(
#   sites$site_no,
#   ~nhdplusTools::discover_nhdplus_id(
#     nldi_feature = list(featureSource = "nwissite",
#                         featureID = paste0("USGS-",.x))) 
# )
# #note single HUC10 is n-COMIDs by n-days in 1990-2021
# sites$huc10 <- c(
#   1710010605, #naselle
#   1710010603 #willapa
#   )
# 
# stp <- map_df(
#   sites$huc10,
#   ~read_csv(paste0("~/T/DFW-Team WDFW Watershed Synthesis - data_common/st_pred/st_pred_171001/st_pred_",.x,".csv")) |> 
#   select(date = tim.date, COMID, st_pred = prd.stream_temp)
#   ) |> 
#   drop_na(st_pred)
# distinct(stp, COMID) |> semi_join(sites, by = "COMID")
# 
# # #do not want HUC10 'average' steam temp across COMIDs
# # #but could also take max daily pred across spatial range
# # #would be better to add HUC10 stratification, mutate in col in map_df
# # stp |> 
# #   group_by(date) |> 
# #   summarise(st_pred_max = max(st_pred, na.rm = T))
# 
# left_join(
#   as_tibble(sites), stp, by = "COMID"
# ) |> 
#   saveRDS("data/nwfsc_st_pred.rds")

```

```{r data_rebuild_rr, eval=FALSE}
#check in with Evan and Colt about compiled estimates from CreelAnalysis, but not in Willapa
#first part is currently unused read from CRC mdb, which has both Chin and coho but only FW & M spt
#second currently used is from coho RR maintained by BM, compiles comm catch and escapement (reapportioned to river)

# mdb_file_path <- "~/T/DFW-Team WDFW Watershed Synthesis - data_common/crc/Sport Harvest Estimates 20230213.mdb"
# 
# crc <- inner_join(
#   readr::read_csv(I(
#     system2(
#       "mdb-export",
#       args = paste(str_replace_all(mdb_file_path, " ", "\\\\ "),"Area"),
#       stdout = T) 
#   ))
#   ,
#   readr::read_csv(I(
#     system2(
#       "mdb-export",
#       args = paste(str_replace_all(mdb_file_path, " ", "\\\\ "),"Catch"),
#       stdout = T) 
#   ))
#   ,  by = "AreaID"
# ) |> 
#   select(
#     AreaCode, AreaName, AreaType, AreaWRIA,
#     CatchYear, CatchStatMonth, Species, CatchEst #, CatchVariance?
#   ) |> 
#   rename_with(~tolower(.) |> str_remove("catch")) |> 
#   filter(
#     #year >= 2000,
#     species %in% c("Coho","Chinook")
#   )
# 
# crc |> 
#   #filter(str_detect(areaname, "illap|aselle")) |> count(areacode, areaname)
#   filter(
#     areacode %in% c("21","375","424"),
#     between(statmonth, 6, 10),
#     !(areacode == "21" & year == 1992) #duplication, 
#     ) |> 
#   #  pivot_wider(names_from = species, values_from = est)
#   ggplot() + geom_col(aes(year, est, fill = statmonth)) + 
#   facet_wrap(~areaname + species, ncol = 3, dir = "v")


#Only have coho RR on hand
f<-"../2024 WB4 Coho Run Reconstruction Model Draft 02.01.2024.xlsx"
# #Marine sport 2.1 is split between pre-2010 in colAY and 2010onward in colP row 73 down
# bind_cols(
#   readxl::read_excel(f, range = "Catch!A9:A52", col_names = "year", col_types = "numeric"),
#   readxl::read_excel(f, range = "Catch!K9:K52", col_names = "naselle_r", col_types = "numeric", na = "Total"),
#   readxl::read_excel(f, range = "Catch!AJ9:AJ52", col_names = "willapa_r", col_types = "numeric", na = "Total")
# ) |> 
#   drop_na(year)

rr_wb_coho <- bind_cols(
  readxl::read_excel(f, range = "System Escapements!Q45:Q72", col_names = "year", col_types = "numeric"),

  readxl::read_excel(f, range = "System Escapements!AC80:AC107", col_names = "naselle_escp", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AF80:AF107", col_names = "naselle_fspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AI80:AI107", col_names = "naselle_mspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AL80:AL107", col_names = "naselle_comm", col_types = "numeric")
  ,
  readxl::read_excel(f, range = "System Escapements!X45:X72", col_names = "willapa_escp", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AA45:AA72", col_names = "willapa_fspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AD45:AD72", col_names = "willapa_mspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AG45:AG72", col_names = "willapa_comm", col_types = "numeric")
)

#the LU to site could use something more generic...
rr_wb_coho |>
  pivot_longer(-year, values_to = "val") |> 
  separate(name, into = c("river","var")) |> 
  mutate(
    site_no = if_else(str_detect(river, "aselle"), "12010000", "12013500")
    ) |> 
  saveRDS("data/rr_wb_coho.rds")

```

# About

## Row

::: {.card title="Daily mean Q + BFS forecast"}
This card displays mean daily streamflow per-day-of-year overlaid by year, with the median per day across years for reference.

In addition, the USGS WA Water Science Center has recently developed near-term [baseflow forecasts ](https://www.usgs.gov/tools/baseflow-forecasts-selected-sites-united-states), and the [current predictions](https://wa.water.usgs.gov/projects/baseflows/BFS_downloads_index.html) are shown with confidence intervals.

As for all other cards:

  - click the lower right corner to expand the card
  
  - double-click on any legend entry to highlight it (then single-click others to add individually or double-click again to return all) 
  
  - zoom to any area of interest.
:::

## Row

::: {.card title="Annual min. daily mean Q"}
This card displays per-year minimum values of daily mean streamflow volume in cfs (cubic feet per second) as measured at the USGS gaging stations (`r paste(unlist(unite(sites, col = "nn", sep = " ")), collapse = " & ")`).

[`r sites$site_no[1]`](`r paste0('https://waterdata.usgs.gov/monitoring-location/',sites$site_no[1],'/#parameterCode=00060&period=P365D&showMedian=true')`)

[`r sites$site_no[2]`](`r paste0('https://waterdata.usgs.gov/monitoring-location/',sites$site_no[2],'/#parameterCode=00060&period=P365D&showMedian=true')`)

:::

::: {.card title="Annual low flow quantiles - Q10, Q05, Q01"}
Based on the same data as the annual minimum, per-year sample quantiles of daily streamflow are shown to illustrate longer term relative differences in several magnitudes of 'low' flow. For example, the dark blue 'Q10' shown here represents the value at which ~90% of observed daily mean flows in a year were greater and ~10% were lower. 
:::

::: {.card title="Annual max pred. daily stream temp, Siegel et al. 2023"}
This card displays per-year maximum values of the [Siegel et al. 2023](https://journals.plos.org/water/article?id=10.1371/journal.pwat.0000119) estimated daily stream temperature for the medium resolution NHD+ COMID (flowline+local catchment) associated with the displayed USGS streamflow gage.

Fitting to the NorWeST database, "This model reflects mechanistic processes using publicly available climate and landscape covariates in a Generalized Additive Model framework. We allowed covariates to interact while accounting for nonlinear relationships between temporal and spatial covariates to better capture seasonal patterns."

Values displayed here are from the [publicly available datasets of results](https://zenodo.org/records/8174951).
:::

::: {.card title="Annual days above degC, Siegel et al. 2023"}
Based on the same data as the annual maximum estimated stream temp displayed above, this card shows the number of days per year with values greater than 18 (yellow) and 20 (orange) degrees C.

This tally is a simple index of relative differences through time, and is *not* equivalent to the 7-DADMax used for [WQS aquatic life temperature criteria](https://app.leg.wa.gov/WAC/default.aspx?cite=173-201A-200) and [303(d) assesments](https://ecology.wa.gov/Water-Shorelines/Water-quality/Water-improvement/Assessment-of-state-waters-303d) as [discoverable elsewhere](https://mywaterway.epa.gov/state/WA/advanced-search).
:::


```{r, include=FALSE}
i <- 1
```

# `r unite(sites,"no_nm")$no_nm[i]`

## Row

```{r}
#| title: Daily mean Q + BFS forecast
plot_q_dv(site = sites$site_no[i], log10 = T)
```

## Row

```{r}
#| title: Annual min. daily mean Q
plot_qmin(sites$site_no[i])
```

```{r}
#| title: Annual low flow quantiles - Q10, Q05, Q01
plot_qquant(sites$site_no[i])
```

```{r}
#| title: Annual max pred. daily stream temp, Siegel et al. 2023
plot_tmax(sites$site_no[i])
```

```{r}
#| title: Annual days above degC, Siegel et al. 2023
plot_tover(sites$site_no[i])
```


```{r, include=FALSE}
i <- 2
```

# `r unite(sites,"no_nm")$no_nm[i]`

## Row

```{r}
#| title: Daily mean Q + BFS forecast
plot_q_dv(site = sites$site_no[i], log10 = T)
```

## Row

```{r}
#| title: Annual min. daily mean Q
plot_qmin(sites$site_no[i])
```

```{r}
#| title: Annual low flow quantiles - Q10, Q05, Q01
plot_qquant(sites$site_no[i])
```

```{r}
#| title: Annual max pred. daily stream temp, Siegel et al. 2023
plot_tmax(sites$site_no[i])
```

```{r}
#| title: Annual days above degC, Siegel et al. 2023
plot_tover(sites$site_no[i])
```
