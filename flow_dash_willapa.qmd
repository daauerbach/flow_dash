---
title: "FLOWdash Willapa"
logo: assets/wdfw_logo_stacked_fullcolor.png
format: 
  dashboard:
    orientation: columns
theme: lux
embed-resources: true

---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
#knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 13)

library("tidyverse", quietly = T)
theme_set(theme_minimal()) 

# #workbook of basic site/station metadata: `usgs_sites`
# readxl::read_excel("~/T/DFW-Team WDFW Watershed Synthesis - flow_trees_heat/usgs_sites_dailyQ_focal.xlsx") |> 
#   select(site_no, station_nm) |> 
#   mutate(station_nm = str_remove(station_nm, ", WA$") |> str_replace("RIVER", "R") |> str_to_title()) 

sites <- data.frame(
  site_no = c("12010000","12013500"),
  station_nm = c("Naselle R Near Naselle", "Willapa R Near Willapa")
)
  
q_obs_range <- c(as.Date("1979-01-01"), Sys.Date())

#build uniform object inserting NAs for missing obs during q_obs_range
#rebuild/overwrite y, m, and yday since `complete` inserts many NAs
#then calc per site per CALENDAR year 
# - cumulative sum of daily mean flow (weird but useful proxy for overall annual volume, 'wet/dry year')
# - 7day moving average of daily mean flow
usgs_q_dv <- list.files("data", pattern = "usgs_dailyQ", full.names = T) |> 
  map_df(~readRDS(.x)) |> 
  #group_by(site_no) |> summarise(dmin = min(date), dmax = max(date))
  tidyr::complete(site_no, date = full_seq(q_obs_range, 1))|> 
  mutate(
    year = as.character(year(date)), month = month(date), yday = yday(date)
    #,q_dv_mean = if_else(q_dv_mean < 0, NA_real_, q_dv_mean)
  ) |> 
  # mutate(
  #   q_dv_mean_sum = cumsum(q_dv_mean),
  #   #,q_dv_mean_7d = slider::slide_dbl(q_dv_mean, ~mean(., na.rm=T), .before = 3, .after = 3),
  #   .by = c(site_no, year)
  # ) |> 
  left_join(sites, by = "site_no") |> 
  select(site_no, station_nm, everything())
#add yday median across years of daily val
#could do 7day of this for pretty 'smooth' reference
usgs_q_dv <- bind_rows(
  usgs_q_dv,
  usgs_q_dv |>
    group_by(site_no, yday) |>
    summarise(
      year = "median",
      across(
        starts_with("q_dv"),
        ~median(.,na.rm=T)
      ),
      .groups = "drop"
    )
)

usgs_bfs <- readRDS("data/usgs_bfs_pred.rds")

nwfsc_st <- readRDS("data/nwfsc_st_pred.rds") |> 
  mutate(
    year = as.character(year(date)), month = month(date), yday = yday(date)
  )

rr_wb_coho <- readRDS("data/rr_wb_coho.rds")


plot_q_dv <- function(site, log10 = T){

  d <- usgs_q_dv |> filter(site_no==site) |> select(year, yday, q_dv_mean)

  p <- plotly::plot_ly(d) |>
    plotly::layout(
      xaxis = list(title='Day of year'),
      yaxis = list(title='cfs')
    )

  for(i in unique(d$year)){
    p <- plotly::add_lines(
      p, name = i, x = ~yday, y = ~q_dv_mean,
      data = filter(d, year == i)
    )
  }

  if(site %in% unique(usgs_bfs$site_no)){
    p <- plotly::add_ribbons(
      p, name = "BFS_predCI", x = ~yday,
      ymin = ~q_bfs_pred05, ymax = ~q_bfs_pred95,
      line = list(dash = "dash", color = "#505050FF", alpha = 0.2),
      data = filter(usgs_bfs, site_no == site)
    )
    p <- plotly::add_lines(
      p, name = "BFS_pred", x = ~yday, y = ~q_bfs_pred,
      line = list(dash = "dash", color = "#000000FF"),
      data = filter(usgs_bfs, site_no == site)
    )
  }

  if (log10){
    p <- p |> plotly::layout(yaxis = list(type = "log"))
    }

  p
}

plot_qmin <- function(site){
  usgs_q_dv |> 
    filter(site_no == site, year != "2024") |>
    slice_min(order_by = q_dv_mean, n = 1, by = year, with_ties = F) |>
    mutate(month = if_else(is.na(month), "med", month.abb[month]) |> 
             factor(levels = c(month.abb[6:10], "med"))) |> 
    select(year, month, min_cfs = q_dv_mean) |> 
    plotly::plot_ly(type = "bar", x = ~year, y = ~min_cfs, color = ~month) |> 
    plotly::rangeslider() |> 
    plotly::layout(
      legend = list(title='month'),
      xaxis = list(title='')
    )  
}

plot_tmax <- function(site){
  nwfsc_st |> 
    filter(site_no == site) |> 
    slice_max(order_by = st_pred, n = 1, by = year, with_ties = F) |>
    mutate(month = if_else(is.na(month), "med", month.abb[month]) |> 
             factor(levels = c(month.abb[6:10], "med"))) |> 
    select(year, month, max_degC = st_pred) |> 
    plotly::plot_ly( type = "bar", x = ~year, y = ~max_degC, color = ~month) |> 
    plotly::rangeslider() |> 
    plotly::layout(
      legend = list(title='month'),
      xaxis = list(title='')
    )  
}

plot_rr_ts <- function(site){
  rr_wb_coho |> 
    filter(site_no == site) |> 
    plotly::plot_ly(
      type = "scatter", mode = "lines+markers",
      x = ~year, y = ~val, color = ~var
      ) |> 
#    plotly::rangeslider() |> 
    plotly::layout(
      legend = list(title=''),
      xaxis = list(title=''),
      yaxis = list(title='')
    ) 
}

#this grabbing flow on warmest day
#could should streamtemp on day of lowest flow?
plot_t_vs_q <- function(site){
  d <- left_join(
    nwfsc_st |> 
      filter(site_no == site) |> 
      slice_max(order_by = st_pred, n = 1, by = year, with_ties = F)
    , 
    usgs_q_dv
    ,
    by = c("site_no","station_nm","date","year","month","yday")    
  ) |>
    inner_join(
      rr_wb_coho |>
        filter(site_no == site 
               #,var == "fspt" 
               #,var %in% c("escp","fspt")
               ) |>
        mutate(year = as.character(year))
      , by = c("site_no", "year")
    ) |> 
    mutate(
      across(where(is.numeric), ~round(., digits = 1))
    )
  
  d |>
    plotly::plot_ly(
      type = "scatter", mode = "markers"
      ) |> 
    plotly::add_markers(
      x = ~q_dv_mean, y = ~st_pred,
      name = ~var, size = ~val, 
      marker = list(sizeref = 0.1),
      text = ~paste(
        '</br> Year: ', year,
        '</br> cfs: ', q_dv_mean,
        '</br> degC: ', st_pred,
        '</br> count: ', val
        )
      ) |> 
    plotly::layout(
      xaxis = list(title='cfs of obs Q on day of max pred. T'),
      yaxis = list(title='degC max pred. T')
    ) 
  
  # p <- map(
  #   c("escp","fspt"),
  #   function(v) {
  #     plotly::plot_ly(
  #       data = filter(d, var == v),
  #       x = ~q_dv_mean, y = ~st_pred, 
  #       name = v,
  #       size = ~val, text = ~year, 
  #       marker = list(sizeref = 0.1)
  #     ) #|> plotly::add_markers(name = v)
  #   })
  # 
  # plotly::subplot(p, nrows = 2)

}
#plot_t_vs_q(sites$site_no[1])

```

```{r data_rebuild_q_dv, eval=FALSE}
# #not `complete()` here since no reason to store potentially lots of NA for long qobsrange
# walk(
#   sites$site_no
#   ,
#   ~dataRetrieval::readNWISdv(
#     .x, parameterCd = "00060",
#     startDate = q_obs_range[1],
#     endDate = q_obs_range[2]
#     ) |>
#     as_tibble() |>
#     mutate(year = year(Date), month = month(Date), yday = yday(Date)) |>
#     select(site_no, date = Date, year, month, yday, q_dv_mean = X_00060_00003) |>
#     saveRDS(paste0("data/usgs_dailyQ_", .x,".rds"))
# )

```

```{r data_rebuild_bfs, eval=FALSE}
# #add new USGS baseflow predictions...
# huc4 <- c("1710","1711")
# url <- paste0("https://wa.water.usgs.gov/projects/baseflows/out/bfprj_HUC",huc4,".csv")
# usgs_bfs <- map_df(url, ~readr::read_csv(.x)) |> 
#   filter(SiteID %in% sites$site_no) |> 
#   mutate(yday = yday(Date)) |> 
#   select(
#     site_no = SiteID, date = Date, yday,
#     q_bfs_pred = Baseflow.cfs,
#     q_bfs_pred05 = StreamflowCB05.cfs, 
#     q_bfs_pred95 = StreamflowCB95.cfs
#     ) |> 
#     saveRDS(paste0("data/usgs_bfs_pred.rds"))
#   
```

```{r data_rebuild_st_pred, eval=FALSE}
#could also think about adding/integrating NWM via AWS flow_trees_apps.qmd>>nwm_zarr_pull2 

# # #need to know gage HUC10 to figure out model prediction file; h10 lu not yet scripted, quick manual via
# # https://geo.wa.gov/datasets/waecy::national-watershed-boundary-dataset-wbd-hydrologic-unit-code-10-digit-basins-of-washington-state/explore?layer=11
# # # then need to know gage COMID since values are by COMID-day
# # # but gage may or may not actually be in the dataset for ... reasons?
# sites$COMID <- map_int(
#   sites$site_no,
#   ~nhdplusTools::discover_nhdplus_id(
#     nldi_feature = list(featureSource = "nwissite",
#                         featureID = paste0("USGS-",.x))) 
# )
# #note single HUC10 is n-COMIDs by n-days in 1990-2021
# sites$huc10 <- c(
#   1710010605, #naselle
#   1710010603 #willapa
#   )
# 
# stp <- map_df(
#   sites$huc10,
#   ~read_csv(paste0("~/T/DFW-Team WDFW Watershed Synthesis - data_common/st_pred/st_pred_171001/st_pred_",.x,".csv")) |> 
#   select(date = tim.date, COMID, st_pred = prd.stream_temp)
#   ) |> 
#   drop_na(st_pred)
# distinct(stp, COMID) |> semi_join(sites, by = "COMID")
# 
# # #do not want HUC10 'average' steam temp across COMIDs
# # #but could also take max daily pred across spatial range
# # #would be better to add HUC10 stratification, mutate in col in map_df
# # stp |> 
# #   group_by(date) |> 
# #   summarise(st_pred_max = max(st_pred, na.rm = T))
# 
# left_join(
#   as_tibble(sites), stp, by = "COMID"
# ) |> 
#   saveRDS("data/nwfsc_st_pred.rds")

```

```{r data_rebuild_rr, eval=FALSE}
#check in with Evan and Colt about compiled estimates from CreelAnalysis, but not in Willapa
#first part is currently unused read from CRC mdb, which has both Chin and coho but only FW & M spt
#second currently used is from coho RR maintained by BM, compiles comm catch and escapement (reapportioned to river)

# mdb_file_path <- "~/T/DFW-Team WDFW Watershed Synthesis - data_common/crc/Sport Harvest Estimates 20230213.mdb"
# 
# crc <- inner_join(
#   readr::read_csv(I(
#     system2(
#       "mdb-export",
#       args = paste(str_replace_all(mdb_file_path, " ", "\\\\ "),"Area"),
#       stdout = T) 
#   ))
#   ,
#   readr::read_csv(I(
#     system2(
#       "mdb-export",
#       args = paste(str_replace_all(mdb_file_path, " ", "\\\\ "),"Catch"),
#       stdout = T) 
#   ))
#   ,  by = "AreaID"
# ) |> 
#   select(
#     AreaCode, AreaName, AreaType, AreaWRIA,
#     CatchYear, CatchStatMonth, Species, CatchEst #, CatchVariance?
#   ) |> 
#   rename_with(~tolower(.) |> str_remove("catch")) |> 
#   filter(
#     #year >= 2000,
#     species %in% c("Coho","Chinook")
#   )
# 
# crc |> 
#   #filter(str_detect(areaname, "illap|aselle")) |> count(areacode, areaname)
#   filter(
#     areacode %in% c("21","375","424"),
#     between(statmonth, 6, 10),
#     !(areacode == "21" & year == 1992) #duplication, 
#     ) |> 
#   #  pivot_wider(names_from = species, values_from = est)
#   ggplot() + geom_col(aes(year, est, fill = statmonth)) + 
#   facet_wrap(~areaname + species, ncol = 3, dir = "v")


#Only have coho RR on hand
f<-"../2024 WB4 Coho Run Reconstruction Model Draft 02.01.2024.xlsx"
# #Marine sport 2.1 is split between pre-2010 in colAY and 2010onward in colP row 73 down
# bind_cols(
#   readxl::read_excel(f, range = "Catch!A9:A52", col_names = "year", col_types = "numeric"),
#   readxl::read_excel(f, range = "Catch!K9:K52", col_names = "naselle_r", col_types = "numeric", na = "Total"),
#   readxl::read_excel(f, range = "Catch!AJ9:AJ52", col_names = "willapa_r", col_types = "numeric", na = "Total")
# ) |> 
#   drop_na(year)

rr_wb_coho <- bind_cols(
  readxl::read_excel(f, range = "System Escapements!Q45:Q72", col_names = "year", col_types = "numeric"),

  readxl::read_excel(f, range = "System Escapements!AC80:AC107", col_names = "naselle_escp", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AF80:AF107", col_names = "naselle_fspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AI80:AI107", col_names = "naselle_mspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AL80:AL107", col_names = "naselle_comm", col_types = "numeric")
  ,
  readxl::read_excel(f, range = "System Escapements!X45:X72", col_names = "willapa_escp", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AA45:AA72", col_names = "willapa_fspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AD45:AD72", col_names = "willapa_mspt", col_types = "numeric"),
  readxl::read_excel(f, range = "System Escapements!AG45:AG72", col_names = "willapa_comm", col_types = "numeric")
)

#the LU to site could use something more generic...
rr_wb_coho |>
  pivot_longer(-year, values_to = "val") |> 
  separate(name, into = c("river","var")) |> 
  mutate(
    site_no = if_else(str_detect(river, "aselle"), "12010000", "12013500")
    ) |> 
  saveRDS("data/rr_wb_coho.rds")

```


# `r unite(sites,"no_nm")$no_nm[1]`

## Column

### Row 

```{r}
#| title: Mean daily Q
plot_q_dv(site = sites$site_no[1], log10 = T)
```

### Row

```{r}
#| title: Annual min Obs mean daily Q
plot_qmin(sites$site_no[1])
```

```{r}
#| title: Annual max Pred daily T
plot_tmax(sites$site_no[1])
```

## Column

```{r}
#| title: Harvest + Escapement
plot_rr_ts(sites$site_no[1])
```

```{r}
#| title: Pred. degC vs obs cfs on day of max T
plot_t_vs_q(sites$site_no[1])
```


# `r unite(sites,"no_nm")$no_nm[2]`

## Column

### Row

```{r}
#| title: Mean daily Q
plot_q_dv(site = sites$site_no[2], log10 = T)
```

### Row

```{r}
#| title: Annual min Obs mean daily Q
plot_qmin(sites$site_no[2])
```

```{r}
#| title: Annual max Pred daily T
plot_tmax(sites$site_no[2])
```

## Column

```{r}
#| title: Harvest + Escapement
plot_rr_ts(sites$site_no[2])
```

```{r}
#| title: Pred. degC vs obs cfs on day of max T
plot_t_vs_q(sites$site_no[2])
```

